{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a3fee85",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "path_to_zip = tf.keras.utils.get_file(\"bike_sharing_dataset.zip\", \"https://archive.ics.uci.edu/static/public/275/bike+sharing+dataset.zip\", cache_dir=\".\", extract=True)\n",
    "extracted_dir = os.path.splitext(path_to_zip)[0] + \"_extracted\"\n",
    "source_csv = os.path.join(\"datasets\", \"bike_sharing_dataset_extracted\", \"hour.csv\")\n",
    "target_csv = os.path.join(\"datasets\", \"hour.csv\")\n",
    "\n",
    "os.makedirs(\"datasets\", exist_ok=True)\n",
    "\n",
    "shutil.copy(source_csv, target_csv)\n",
    "\n",
    "print(f\"Plik hour.csv zosta≈Ç skopiowany do {target_csv}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "451a50c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('datasets/bike_sharing_dataset_extracted/hour.csv')\n",
    "df['datetime'] = pd.to_datetime(df['dteday'] + ' ' + df['hr'].astype(str).str.zfill(2), format='%Y-%m-%d %H')\n",
    "\n",
    "df.set_index('datetime', inplace=True)\n",
    "\n",
    "print((df.index.min(), df.index.max()))\n",
    "print((365 + 366) * 24 - len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b92cd3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_index = pd.date_range(start=df.index.min(), end=df.index.max(), freq='H')\n",
    "missing = full_index.difference(df.index)\n",
    "print(missing)\n",
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e12ded8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "resampled = pd.DataFrame()\n",
    "resampled[['holiday', 'weekday', 'workingday', 'weathersit']] = df.resample(rule='1h').ffill()[['holiday', 'weekday', 'workingday', 'weathersit']]\n",
    "resampled[['temp', 'atemp', 'hum', 'windspeed']] = df.resample(rule='1h').interpolate(method=\"linear\")[['temp', 'atemp', 'hum', 'windspeed']]\n",
    "resampled[['casual', 'registered', 'cnt']] = df.resample(rule='1h').asfreq(0)[['casual', 'registered', 'cnt']]\n",
    "# print(df.loc[\"2011-01-02 00:00:00\":\"2011-01-02 10:00:00\"])\n",
    "print(resampled.loc[\"2011-01-02 03:00:00\":\"2011-01-02 7:00:00\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7e6baaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = resampled\n",
    "df.notna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d967097",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['casual', 'registered', 'cnt', 'weathersit']].describe()\n",
    "\n",
    "df.casual /= 1e3\n",
    "df.registered /= 1e3\n",
    "df.cnt /= 1e3\n",
    "df.weathersit /= 4\n",
    "\n",
    "df_2weeks = df[:24 * 7 * 2]\n",
    "df_2weeks[['casual', 'registered', 'cnt', 'temp']].plot(figsize=(10, 3))\n",
    "\n",
    "df_daily = df.resample('W').mean()\n",
    "df_daily[['casual', 'registered', 'cnt', 'temp']].plot(figsize=(10, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9caf48d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "df['cnt_scaled'] = df['cnt'] * 10**3\n",
    "\n",
    "cnt_pred_day = df['cnt_scaled'].shift(24)\n",
    "mae_daily = np.mean(np.abs(df['cnt_scaled'] - cnt_pred_day))\n",
    "\n",
    "cnt_pred_week = df['cnt_scaled'].shift(24 * 7)\n",
    "mae_weekly = np.mean(np.abs(df['cnt_scaled'] - cnt_pred_week))\n",
    "\n",
    "mae_daily = round(mae_daily, 2)\n",
    "mae_weekly = round(mae_weekly, 2)\n",
    "\n",
    "with open(\"mae_baseline.pkl\", \"wb\") as f:\n",
    "    pickle.dump((mae_daily, mae_weekly), f)\n",
    "\n",
    "print(f\"MAE daily: {mae_daily}, MAE weekly: {mae_weekly}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed4993bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt_train = df['cnt']['2011-01-01 00:00':'2012-06-30 23:00']\n",
    "cnt_valid = df['cnt']['2012-07-01 00:00':]\n",
    "\n",
    "seq_len = 1 * 24\n",
    "train_ds = tf.keras.utils.timeseries_dataset_from_array(\n",
    "            cnt_train.to_numpy(),\n",
    "            targets=cnt_train[seq_len:],\n",
    "            sequence_length=seq_len,\n",
    "            batch_size=32,\n",
    "            shuffle=True,\n",
    "            seed=42\n",
    "            )\n",
    "\n",
    "valid_ds = tf.keras.utils.timeseries_dataset_from_array(\n",
    "            cnt_valid.to_numpy(),\n",
    "            targets=cnt_valid[seq_len:],\n",
    "            sequence_length=seq_len,\n",
    "            batch_size=32\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5649758c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras \n",
    "\n",
    "def build_model(learning_rate):\n",
    "    model = tf.keras.Sequential([tf.keras.Input(shape=(seq_len,)), tf.keras.layers.Dense(1)])\n",
    "    model.compile(optimizer=keras.optimizers.SGD(learning_rate=learning_rate, momentum=0.9), loss=keras.losses.Huber(), metrics=['mae'])\n",
    "    return model\n",
    "\n",
    "def build_model_LSTM(learning_rate):\n",
    "    model = tf.keras.Sequential([tf.keras.Input(shape=[None, 1]), tf.keras.layers.LSTM(1)])\n",
    "    model.compile(optimizer=keras.optimizers.SGD(learning_rate=learning_rate, momentum=0.9), loss=keras.losses.Huber(), metrics=['mae'])\n",
    "    return model\n",
    "\n",
    "def build_model_RNN(learning_rate):\n",
    "    model = tf.keras.Sequential([tf.keras.Input(shape=[None, 1]), tf.keras.layers.LSTM(32), tf.keras.layers.Dense(1)])\n",
    "    model.compile(optimizer=keras.optimizers.SGD(learning_rate=learning_rate, momentum=0.9), loss=keras.losses.Huber(), metrics=['mae'])\n",
    "    return model\n",
    "\n",
    "def build_model_DEEP_RNN(learning_rate):\n",
    "    model = tf.keras.Sequential([tf.keras.Input(shape=[None, 1])])\n",
    "    model.add(tf.keras.layers.LSTM(32, return_sequences=True))\n",
    "    model.add(tf.keras.layers.LSTM(32, return_sequences=True))\n",
    "    model.add(tf.keras.layers.LSTM(32))\n",
    "    model.add(tf.keras.layers.Dense(1))\n",
    "    model.compile(optimizer=keras.optimizers.SGD(learning_rate=learning_rate, momentum=0.9), loss=keras.losses.Huber(), metrics=['mae'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d52c5e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    \"linear\"  : build_model,\n",
    "    \"rnn1\"    : build_model_LSTM,\n",
    "    \"rnn32\"   : build_model_RNN,\n",
    "    \"rnn_deep\": build_model_DEEP_RNN\n",
    "}\n",
    "\n",
    "for name in models.keys():\n",
    "    lr = 0.1\n",
    "    lr_schedule = tf.keras.callbacks.ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.3,\n",
    "        patience=3,\n",
    "        verbose=1,\n",
    "        min_lr=1e-6\n",
    "    )\n",
    "\n",
    "    model = models[name](lr)\n",
    "    model.fit(train_ds, epochs=20, validation_data=valid_ds, verbose=0, callbacks=[lr_schedule])\n",
    "    mea_linear = model.evaluate(valid_ds)[1]\n",
    "    model.save(f'model_{name}.keras')\n",
    "\n",
    "    with open(f\"mae_{name}.pkl\", \"wb\") as file:\n",
    "        pickle.dump((mea_linear * 10**3, ), file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e772b0ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nie rozumiem czemu nie tak jak jest mniejszy mae ale ok\n",
    "# cnt_train = df[['workingday', 'weathersit', 'atemp']]['2011-01-01 00:00':'2012-06-30 23:00']\n",
    "# cnt_train_y = df[['cnt']]['2011-01-01 00:00':'2012-06-30 23:00']\n",
    "# cnt_valid = df[['workingday', 'weathersit', 'atemp']]['2012-07-01 00:00':]\n",
    "# cnt_valid_y = df[['cnt']]['2012-07-01 00:00':]\n",
    "\n",
    "# seq_len = 1 * 24\n",
    "# train_ds = tf.keras.utils.timeseries_dataset_from_array(\n",
    "#             cnt_train.to_numpy(),\n",
    "#             targets=cnt_train_y.to_numpy()[seq_len:],\n",
    "#             sequence_length=seq_len,\n",
    "#             batch_size=32,\n",
    "#             shuffle=True,\n",
    "#             seed=42\n",
    "#             )\n",
    "\n",
    "# valid_ds = tf.keras.utils.timeseries_dataset_from_array(\n",
    "#             cnt_valid.to_numpy(),\n",
    "#             targets=cnt_valid_y.to_numpy()[seq_len:],\n",
    "#             sequence_length=seq_len,\n",
    "#             batch_size=32\n",
    "#             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "214f82cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt_train = df[['cnt', 'workingday', 'weathersit', 'atemp']]['2011-01-01 00:00':'2012-06-30 23:00']\n",
    "cnt_valid = df[['cnt', 'workingday', 'weathersit', 'atemp']]['2012-07-01 00:00':]\n",
    "\n",
    "seq_len = 1 * 24\n",
    "train_ds = tf.keras.utils.timeseries_dataset_from_array(\n",
    "            cnt_train.to_numpy(),\n",
    "            targets=cnt_train[seq_len:],\n",
    "            sequence_length=seq_len,\n",
    "            batch_size=32,\n",
    "            shuffle=True,\n",
    "            seed=42\n",
    "            )\n",
    "\n",
    "valid_ds = tf.keras.utils.timeseries_dataset_from_array(\n",
    "            cnt_valid.to_numpy(),\n",
    "            targets=cnt_valid[seq_len:],\n",
    "            sequence_length=seq_len,\n",
    "            batch_size=32\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c7d752a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras \n",
    "\n",
    "def build_model_deep_rnn_multi(learning_rate):\n",
    "    model = tf.keras.Sequential([tf.keras.Input(shape=[None, 4])])\n",
    "    model.add(tf.keras.layers.LSTM(32))\n",
    "    model.add(tf.keras.layers.Dense(1))\n",
    "    model.compile(optimizer=keras.optimizers.SGD(learning_rate=learning_rate, momentum=0.9), loss=keras.losses.Huber(), metrics=['mae'])\n",
    "    return model\n",
    "\n",
    "lr = 0.1\n",
    "lr_schedule = tf.keras.callbacks.ReduceLROnPlateau(\n",
    "    monitor='val_loss',\n",
    "    factor=0.3,\n",
    "    patience=3,\n",
    "    verbose=1,\n",
    "    min_lr=1e-6\n",
    ")\n",
    "\n",
    "model = build_model_deep_rnn_multi(lr)\n",
    "model.fit(train_ds, epochs=20, validation_data=valid_ds, verbose=0, callbacks=[lr_schedule])\n",
    "mae = model.evaluate(valid_ds)[1]\n",
    "\n",
    "model.save('model_rnn_mv.keras')\n",
    "\n",
    "with open(\"mae_rnn_mv.pkl\", \"wb\") as file:\n",
    "    pickle.dump((mae * 10 **3, ), file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
